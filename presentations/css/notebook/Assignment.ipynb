{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Practical assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this assignment you will analyse user comments from the website [reddit.com](http://www.reddit.com). Reddit users can post content (e.g., a website, a question, news), which can be up- or downvoted. Posts with many upvotes tend to appear in the top of the category or at the frontpage of Reddit. The website is quite popular and has over half a billion monthly visitors. At times, appearing on the frontpage of Reddit generates so much traffic to the posted website, that it actually crashes.\n",
    "\n",
    "The community is organised in various subreddits, such as news, movies, music, et cetera. You will analyse user comments from the [politics subreddit](https://www.reddit.com/r/politics/). These user comments are either replies to the starting post, or replies to other usersâ€™ comments. The latter will be the basis for the communication network that you will construct here.\n",
    "\n",
    "First let us get started with the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "** Data **\n",
    "\n",
    "If you have not done so already, download all data from https://storage.googleapis.com/css-files/reddit_discussion_network_2016_10.csv. This file is 377MB, it may take some time to download. If you have trouble working with this dataset on your computer, please try the alternative: https://storage.googleapis.com/css-files/reddit_discussion_network_2015_02.csv, which is only 46MB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "** Importing libraries **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import igraph as ig\n",
    "import nltk\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Reading in data**\n",
    "\n",
    "First read in Reddit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "file_name = 'reddit_discussion_network_2016_10.csv';\n",
    "df = pd.read_csv('../../../../data/' + file_name);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Which columns does this dataset have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'comment', u'a_score', u'a_created_utc', u'a_retrieved_on',\n",
      "       u'comment_id', u'comment_reply_to_id', u'author_from',\n",
      "       u'author_reply_to'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The first post:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             comment  a_score  a_created_utc  \\\n",
      "0  You think these women were sitting out there m...        0     1476397749   \n",
      "\n",
      "   a_retrieved_on comment_id comment_reply_to_id     author_from  \\\n",
      "0      1478583791    d8qwv4k             d8qwg9k  Schmingleberry   \n",
      "\n",
      "        author_reply_to  \n",
      "0  pm_me_your_cuck_pics  \n"
     ]
    }
   ],
   "source": [
    "print df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You can convert the dataframe into a graph using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "G = ig.Graph.DictList(\n",
    "        vertices=None,\n",
    "        edges=df.to_dict('records'),\n",
    "        directed=True,\n",
    "        edge_foreign_keys=('author_from', 'author_reply_to'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You can simplify the graph by using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "G.simplify(combine_edges={'comment': len});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This simply counts the length of the list of edges between the same pair of nodes.\n",
    "Any value that you calculate for a comment (e.g. a topic or a sentiment) can be added to the dataframe and can be used in the construction of the graph. \n",
    "\n",
    "There are now four smaller subassignments which we will work on. You can choose any single one to work on. Hints for doing some of the analysis are provided after the description of the subassignments. Most of the techniques involved should already be explained during the lectures, but these hints provide some more explicit help.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Topics and centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Users that are central tend to interact with lots of different (central) users. We could either expect that users become more central if they secure a position of authority in a single topic. In that case, everybody interacts with the user because he is authoritative in this subject. Alternatively, somebody can be more central because he is active in many different topics. Finally, somebody may simply be more central because he is active himself, and every comment is likely to get a reply.\n",
    "\n",
    "Techniques necessary\n",
    "- Topic detection\n",
    "- Centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Read in trained LDA model and dictionary (identifier to word)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lda_model_reddit = gensim.models.ldamodel.LdaModel.load('filename')\n",
    "id2word_reddit = gensim.corpora.dictionary.Dictionary.load(\"filename\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help speed up the analysis, we already computed topic values for each post. You can read the data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_sentiment_df = pd.read_csv('../../../../data/' + 'topic_sentiment_reddit.csv');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each post, the topic distribution is saved in t_0 to t_14 (15 topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  comment_id     author_from     pos  neg       t_0       t_1       t_2  \\\n",
      "0    d8qwv4k  Schmingleberry  0.0125  0.0  0.001667  0.150760  0.001667   \n",
      "1    d8p5x7o    socoamaretto  0.0000  0.0  0.016667  0.016667  0.016667   \n",
      "2    d9dpj9r    allisslothed  0.0000  0.0  0.033333  0.033333  0.033333   \n",
      "3    d8lh0lw    shaking_head  0.0250  0.0  0.002899  0.002899  0.046468   \n",
      "4    d8cu7q1        InFearn0  0.0000  0.0  0.338153  0.001515  0.001515   \n",
      "\n",
      "        t_3       t_4       t_5       t_6       t_7       t_8       t_9  \\\n",
      "0  0.001667  0.170236  0.001667  0.058648  0.001667  0.001667  0.146286   \n",
      "1  0.016667  0.016667  0.016667  0.016667  0.766667  0.016667  0.016667   \n",
      "2  0.033333  0.033333  0.033333  0.033333  0.033333  0.033333  0.033333   \n",
      "3  0.448935  0.131211  0.002899  0.002899  0.156079  0.002899  0.072449   \n",
      "4  0.001515  0.060315  0.038145  0.001515  0.001515  0.032549  0.001515   \n",
      "\n",
      "       t_10      t_11      t_12      t_13      t_14  \n",
      "0  0.164797  0.001667  0.109014  0.061628  0.126965  \n",
      "1  0.016667  0.016667  0.016667  0.016667  0.016667  \n",
      "2  0.033333  0.033333  0.533333  0.033333  0.033334  \n",
      "3  0.002899  0.002899  0.118771  0.002899  0.002899  \n",
      "4  0.112769  0.152212  0.060566  0.194684  0.001515  \n"
     ]
    }
   ],
   "source": [
    "print topic_sentiment_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now calculate the average values for each user as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_sentiment_user_df = topic_sentiment_df.groupby(['author_from'], as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_from</th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "      <th>t_0</th>\n",
       "      <th>t_1</th>\n",
       "      <th>t_2</th>\n",
       "      <th>t_3</th>\n",
       "      <th>t_4</th>\n",
       "      <th>t_5</th>\n",
       "      <th>t_6</th>\n",
       "      <th>t_7</th>\n",
       "      <th>t_8</th>\n",
       "      <th>t_9</th>\n",
       "      <th>t_10</th>\n",
       "      <th>t_11</th>\n",
       "      <th>t_12</th>\n",
       "      <th>t_13</th>\n",
       "      <th>t_14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--------Link--------</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.262674</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.056574</td>\n",
       "      <td>0.069515</td>\n",
       "      <td>0.520009</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.056140</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.003509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>------________</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.043675</td>\n",
       "      <td>0.017526</td>\n",
       "      <td>0.017526</td>\n",
       "      <td>0.039061</td>\n",
       "      <td>0.096752</td>\n",
       "      <td>0.074879</td>\n",
       "      <td>0.029013</td>\n",
       "      <td>0.190172</td>\n",
       "      <td>0.129516</td>\n",
       "      <td>0.017526</td>\n",
       "      <td>0.035383</td>\n",
       "      <td>0.050420</td>\n",
       "      <td>0.098166</td>\n",
       "      <td>0.151150</td>\n",
       "      <td>0.035383</td>\n",
       "      <td>0.017526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-----iMartijn-----</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.017867</td>\n",
       "      <td>0.018383</td>\n",
       "      <td>0.074263</td>\n",
       "      <td>0.005108</td>\n",
       "      <td>0.230240</td>\n",
       "      <td>0.116360</td>\n",
       "      <td>0.039299</td>\n",
       "      <td>0.017694</td>\n",
       "      <td>0.054546</td>\n",
       "      <td>0.005108</td>\n",
       "      <td>0.005108</td>\n",
       "      <td>0.247091</td>\n",
       "      <td>0.053790</td>\n",
       "      <td>0.102172</td>\n",
       "      <td>0.020444</td>\n",
       "      <td>0.010394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>---CAISSON---</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.009697</td>\n",
       "      <td>0.009697</td>\n",
       "      <td>0.109697</td>\n",
       "      <td>0.009697</td>\n",
       "      <td>0.009697</td>\n",
       "      <td>0.055152</td>\n",
       "      <td>0.009697</td>\n",
       "      <td>0.061478</td>\n",
       "      <td>0.009697</td>\n",
       "      <td>0.109697</td>\n",
       "      <td>0.367007</td>\n",
       "      <td>0.209697</td>\n",
       "      <td>0.009697</td>\n",
       "      <td>0.009697</td>\n",
       "      <td>0.009697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>---DONTDIEWEMULTIPLY</td>\n",
       "      <td>0.005842</td>\n",
       "      <td>0.007025</td>\n",
       "      <td>0.022242</td>\n",
       "      <td>0.154403</td>\n",
       "      <td>0.056291</td>\n",
       "      <td>0.104365</td>\n",
       "      <td>0.082621</td>\n",
       "      <td>0.054479</td>\n",
       "      <td>0.043127</td>\n",
       "      <td>0.086735</td>\n",
       "      <td>0.070458</td>\n",
       "      <td>0.050695</td>\n",
       "      <td>0.081356</td>\n",
       "      <td>0.032482</td>\n",
       "      <td>0.089906</td>\n",
       "      <td>0.057650</td>\n",
       "      <td>0.013191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            author_from       pos       neg       t_0       t_1       t_2  \\\n",
       "0  --------Link--------  0.000000  0.062500  0.262674  0.003509  0.003509   \n",
       "1        ------________  0.017857  0.043675  0.017526  0.017526  0.039061   \n",
       "2    -----iMartijn-----  0.002000  0.017867  0.018383  0.074263  0.005108   \n",
       "3         ---CAISSON---  0.000000  0.025000  0.009697  0.009697  0.109697   \n",
       "4  ---DONTDIEWEMULTIPLY  0.005842  0.007025  0.022242  0.154403  0.056291   \n",
       "\n",
       "        t_3       t_4       t_5       t_6       t_7       t_8       t_9  \\\n",
       "0  0.003509  0.003509  0.056574  0.069515  0.520009  0.003509  0.056140   \n",
       "1  0.096752  0.074879  0.029013  0.190172  0.129516  0.017526  0.035383   \n",
       "2  0.230240  0.116360  0.039299  0.017694  0.054546  0.005108  0.005108   \n",
       "3  0.009697  0.009697  0.055152  0.009697  0.061478  0.009697  0.109697   \n",
       "4  0.104365  0.082621  0.054479  0.043127  0.086735  0.070458  0.050695   \n",
       "\n",
       "       t_10      t_11      t_12      t_13      t_14  \n",
       "0  0.003509  0.003509  0.003509  0.003509  0.003509  \n",
       "1  0.050420  0.098166  0.151150  0.035383  0.017526  \n",
       "2  0.247091  0.053790  0.102172  0.020444  0.010394  \n",
       "3  0.367007  0.209697  0.009697  0.009697  0.009697  \n",
       "4  0.081356  0.032482  0.089906  0.057650  0.013191  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_sentiment_user_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select the values for a particular user, do this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_from</th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "      <th>t_0</th>\n",
       "      <th>t_1</th>\n",
       "      <th>t_2</th>\n",
       "      <th>t_3</th>\n",
       "      <th>t_4</th>\n",
       "      <th>t_5</th>\n",
       "      <th>t_6</th>\n",
       "      <th>t_7</th>\n",
       "      <th>t_8</th>\n",
       "      <th>t_9</th>\n",
       "      <th>t_10</th>\n",
       "      <th>t_11</th>\n",
       "      <th>t_12</th>\n",
       "      <th>t_13</th>\n",
       "      <th>t_14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>---CAISSON---</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.009697</td>\n",
       "      <td>0.009697</td>\n",
       "      <td>0.109697</td>\n",
       "      <td>0.009697</td>\n",
       "      <td>0.009697</td>\n",
       "      <td>0.055152</td>\n",
       "      <td>0.009697</td>\n",
       "      <td>0.061478</td>\n",
       "      <td>0.009697</td>\n",
       "      <td>0.109697</td>\n",
       "      <td>0.367007</td>\n",
       "      <td>0.209697</td>\n",
       "      <td>0.009697</td>\n",
       "      <td>0.009697</td>\n",
       "      <td>0.009697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     author_from  pos    neg       t_0       t_1       t_2       t_3  \\\n",
       "3  ---CAISSON---  0.0  0.025  0.009697  0.009697  0.109697  0.009697   \n",
       "\n",
       "        t_4       t_5       t_6       t_7       t_8       t_9      t_10  \\\n",
       "3  0.009697  0.055152  0.009697  0.061478  0.009697  0.109697  0.367007   \n",
       "\n",
       "       t_11      t_12      t_13      t_14  \n",
       "3  0.209697  0.009697  0.009697  0.009697  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_sentiment_user_df.loc[topic_sentiment_user_df['author_from'] ==  '---CAISSON---']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can easily convert this to for example a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['---CAISSON---',\n",
       "  0.0,\n",
       "  0.025,\n",
       "  0.009696985756390001,\n",
       "  0.009696972080605,\n",
       "  0.109696969609305,\n",
       "  0.0096969706267,\n",
       "  0.009696969877439999,\n",
       "  0.055151522521100005,\n",
       "  0.009696969696954999,\n",
       "  0.06147797673265,\n",
       "  0.009696969894069998,\n",
       "  0.10969696975030499,\n",
       "  0.3670068653565,\n",
       "  0.20969691976345498,\n",
       "  0.009696994830435,\n",
       "  0.009696973806775,\n",
       "  0.009696969696954999]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_sentiment_user_df.loc[topic_sentiment_user_df['author_from'] ==  '---CAISSON---'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "** Compute statistics **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "One way to calculate whether a user is posting mostly about one topic, or is the user is active in multiple topics is using **entropy** (https://en.wikipedia.org/wiki/Entropy_(information_theory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is an example where we have two topics. Because the probability of both topics is equal (0.5), the entropy is high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.69314718056\n"
     ]
    }
   ],
   "source": [
    "print scipy.stats.entropy([0.5, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Because in the following example all the probability is concentrated on one topic, the entropy is low (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print scipy.stats.entropy([1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "** User selection **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For this particular assignment, it might be useful to filter users. If you include *all* users, then users who have only posted a few posts might have a topic distribution skewed towards a few topics, just because they haven't been active much. Let's count the number of posts for each user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "user_post_count = df.groupby('author_from', as_index=False).size().rename('counts').reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Show the first 5 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_from</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--------Link--------</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>------________</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-----iMartijn-----</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>---CAISSON---</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>---DONTDIEWEMULTIPLY</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            author_from  counts\n",
       "0  --------Link--------       1\n",
       "1        ------________      14\n",
       "2    -----iMartijn-----       4\n",
       "3         ---CAISSON---       2\n",
       "4  ---DONTDIEWEMULTIPLY      27"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_post_count.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select users with at least 50 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected_users = list(user_post_count[user_post_count.counts >= 50]['author_from'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample 1000 users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.shuffle(selected_users)\n",
    "selected_users_sampled = selected_users[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "** Centrality ** \n",
    "\n",
    "There are various possible centralities. Betweenness in in too slow to calculate for this network, so we will only focus on eigenvector centrality, pagerank and (in- or out-)degree. You can try any one of them, just keep in mind when interpreting further results. You can get the centralities by running any one of the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[206,\n",
       " 459,\n",
       " 64,\n",
       " 327,\n",
       " 422,\n",
       " 30,\n",
       " 148,\n",
       " 757,\n",
       " 446,\n",
       " 85,\n",
       " 27,\n",
       " 292,\n",
       " 152,\n",
       " 21,\n",
       " 108,\n",
       " 221,\n",
       " 365,\n",
       " 70,\n",
       " 29,\n",
       " 77,\n",
       " 111,\n",
       " 126,\n",
       " 584,\n",
       " 12,\n",
       " 426,\n",
       " 215,\n",
       " 134,\n",
       " 181,\n",
       " 13,\n",
       " 13,\n",
       " 146,\n",
       " 24,\n",
       " 678,\n",
       " 10,\n",
       " 132,\n",
       " 35,\n",
       " 83,\n",
       " 180,\n",
       " 229,\n",
       " 218,\n",
       " 256,\n",
       " 279,\n",
       " 28,\n",
       " 66,\n",
       " 65,\n",
       " 159,\n",
       " 81,\n",
       " 4,\n",
       " 280,\n",
       " 10,\n",
       " 18,\n",
       " 941,\n",
       " 9,\n",
       " 112,\n",
       " 44,\n",
       " 394,\n",
       " 489,\n",
       " 1,\n",
       " 789,\n",
       " 7,\n",
       " 293,\n",
       " 1180,\n",
       " 183,\n",
       " 8,\n",
       " 42,\n",
       " 127,\n",
       " 959,\n",
       " 80,\n",
       " 232,\n",
       " 370,\n",
       " 246,\n",
       " 473,\n",
       " 208,\n",
       " 69,\n",
       " 208,\n",
       " 75,\n",
       " 64,\n",
       " 773,\n",
       " 597,\n",
       " 474,\n",
       " 481,\n",
       " 94,\n",
       " 2,\n",
       " 9,\n",
       " 20,\n",
       " 191,\n",
       " 23,\n",
       " 959,\n",
       " 123,\n",
       " 14,\n",
       " 15,\n",
       " 1571,\n",
       " 1422,\n",
       " 1,\n",
       " 489,\n",
       " 141,\n",
       " 114,\n",
       " 5,\n",
       " 383,\n",
       " 61,\n",
       " 111,\n",
       " 95,\n",
       " 365,\n",
       " 101,\n",
       " 317,\n",
       " 36,\n",
       " 490,\n",
       " 48,\n",
       " 560,\n",
       " 831,\n",
       " 136,\n",
       " 159,\n",
       " 311,\n",
       " 81,\n",
       " 57,\n",
       " 19,\n",
       " 16,\n",
       " 73,\n",
       " 5,\n",
       " 15,\n",
       " 105,\n",
       " 693,\n",
       " 971,\n",
       " 158,\n",
       " 105,\n",
       " 419,\n",
       " 407,\n",
       " 433,\n",
       " 719,\n",
       " 78,\n",
       " 95,\n",
       " 236,\n",
       " 194,\n",
       " 216,\n",
       " 511,\n",
       " 29,\n",
       " 4,\n",
       " 290,\n",
       " 364,\n",
       " 223,\n",
       " 5,\n",
       " 16,\n",
       " 29,\n",
       " 155,\n",
       " 113,\n",
       " 280,\n",
       " 29,\n",
       " 1737,\n",
       " 245,\n",
       " 264,\n",
       " 742,\n",
       " 21,\n",
       " 47,\n",
       " 4,\n",
       " 1885,\n",
       " 113,\n",
       " 273,\n",
       " 127,\n",
       " 9,\n",
       " 489,\n",
       " 327,\n",
       " 160,\n",
       " 24,\n",
       " 24,\n",
       " 25,\n",
       " 42,\n",
       " 768,\n",
       " 18,\n",
       " 5,\n",
       " 166,\n",
       " 86,\n",
       " 20,\n",
       " 182,\n",
       " 959,\n",
       " 22,\n",
       " 123,\n",
       " 79,\n",
       " 132,\n",
       " 78,\n",
       " 970,\n",
       " 183,\n",
       " 23,\n",
       " 121,\n",
       " 8,\n",
       " 21,\n",
       " 342,\n",
       " 118,\n",
       " 2048,\n",
       " 34,\n",
       " 51,\n",
       " 146,\n",
       " 17,\n",
       " 338,\n",
       " 12,\n",
       " 87,\n",
       " 59,\n",
       " 1949,\n",
       " 248,\n",
       " 676,\n",
       " 113,\n",
       " 158,\n",
       " 16,\n",
       " 30,\n",
       " 535,\n",
       " 662,\n",
       " 80,\n",
       " 15,\n",
       " 415,\n",
       " 15,\n",
       " 102,\n",
       " 194,\n",
       " 4,\n",
       " 12,\n",
       " 6,\n",
       " 18,\n",
       " 30,\n",
       " 6,\n",
       " 248,\n",
       " 475,\n",
       " 282,\n",
       " 24,\n",
       " 5,\n",
       " 169,\n",
       " 231,\n",
       " 7,\n",
       " 21,\n",
       " 31,\n",
       " 275,\n",
       " 33,\n",
       " 38,\n",
       " 242,\n",
       " 172,\n",
       " 14,\n",
       " 54,\n",
       " 798,\n",
       " 79,\n",
       " 221,\n",
       " 608,\n",
       " 321,\n",
       " 308,\n",
       " 86,\n",
       " 178,\n",
       " 340,\n",
       " 273,\n",
       " 143,\n",
       " 249,\n",
       " 12,\n",
       " 20,\n",
       " 11,\n",
       " 166,\n",
       " 780,\n",
       " 26,\n",
       " 3649,\n",
       " 32,\n",
       " 54,\n",
       " 1,\n",
       " 351,\n",
       " 361,\n",
       " 85,\n",
       " 9,\n",
       " 127,\n",
       " 121,\n",
       " 356,\n",
       " 126,\n",
       " 20,\n",
       " 128,\n",
       " 99,\n",
       " 82,\n",
       " 6,\n",
       " 12,\n",
       " 69,\n",
       " 360,\n",
       " 1197,\n",
       " 228,\n",
       " 140,\n",
       " 131,\n",
       " 317,\n",
       " 62,\n",
       " 3,\n",
       " 4,\n",
       " 303,\n",
       " 403,\n",
       " 1804,\n",
       " 1,\n",
       " 98,\n",
       " 71,\n",
       " 398,\n",
       " 315,\n",
       " 2234,\n",
       " 48,\n",
       " 95,\n",
       " 744,\n",
       " 1,\n",
       " 166,\n",
       " 36,\n",
       " 473,\n",
       " 55,\n",
       " 3,\n",
       " 353,\n",
       " 23,\n",
       " 141,\n",
       " 103,\n",
       " 271,\n",
       " 1,\n",
       " 1010,\n",
       " 11,\n",
       " 7,\n",
       " 216,\n",
       " 427,\n",
       " 49,\n",
       " 9,\n",
       " 15,\n",
       " 25,\n",
       " 519,\n",
       " 5,\n",
       " 246,\n",
       " 285,\n",
       " 241,\n",
       " 372,\n",
       " 118,\n",
       " 61,\n",
       " 6,\n",
       " 410,\n",
       " 15,\n",
       " 2,\n",
       " 52,\n",
       " 70,\n",
       " 3,\n",
       " 27,\n",
       " 134,\n",
       " 51,\n",
       " 260,\n",
       " 249,\n",
       " 6,\n",
       " 114,\n",
       " 97,\n",
       " 45,\n",
       " 33,\n",
       " 175,\n",
       " 49,\n",
       " 43,\n",
       " 9,\n",
       " 225,\n",
       " 147,\n",
       " 298,\n",
       " 7,\n",
       " 307,\n",
       " 175,\n",
       " 32,\n",
       " 13,\n",
       " 39,\n",
       " 11,\n",
       " 73,\n",
       " 10,\n",
       " 16,\n",
       " 31,\n",
       " 139,\n",
       " 103,\n",
       " 18,\n",
       " 15,\n",
       " 275,\n",
       " 142,\n",
       " 149,\n",
       " 168,\n",
       " 1374,\n",
       " 50,\n",
       " 55,\n",
       " 21,\n",
       " 1499,\n",
       " 135,\n",
       " 56,\n",
       " 19,\n",
       " 59,\n",
       " 16,\n",
       " 218,\n",
       " 8,\n",
       " 9,\n",
       " 190,\n",
       " 271,\n",
       " 47,\n",
       " 47,\n",
       " 54,\n",
       " 1302,\n",
       " 792,\n",
       " 24,\n",
       " 2,\n",
       " 121,\n",
       " 217,\n",
       " 2,\n",
       " 141,\n",
       " 124,\n",
       " 459,\n",
       " 383,\n",
       " 2,\n",
       " 25,\n",
       " 76,\n",
       " 4,\n",
       " 94,\n",
       " 437,\n",
       " 200,\n",
       " 38,\n",
       " 670,\n",
       " 1047,\n",
       " 15,\n",
       " 94,\n",
       " 61,\n",
       " 29,\n",
       " 323,\n",
       " 69,\n",
       " 569,\n",
       " 211,\n",
       " 40,\n",
       " 23,\n",
       " 31,\n",
       " 388,\n",
       " 14,\n",
       " 6,\n",
       " 142,\n",
       " 41,\n",
       " 24,\n",
       " 103,\n",
       " 32,\n",
       " 14,\n",
       " 150,\n",
       " 52,\n",
       " 3,\n",
       " 125,\n",
       " 71,\n",
       " 398,\n",
       " 135,\n",
       " 10,\n",
       " 59,\n",
       " 3,\n",
       " 971,\n",
       " 51,\n",
       " 409,\n",
       " 94,\n",
       " 418,\n",
       " 103,\n",
       " 6,\n",
       " 11,\n",
       " 239,\n",
       " 475,\n",
       " 27,\n",
       " 51,\n",
       " 152,\n",
       " 261,\n",
       " 165,\n",
       " 116,\n",
       " 606,\n",
       " 33,\n",
       " 26,\n",
       " 997,\n",
       " 17,\n",
       " 30,\n",
       " 87,\n",
       " 2,\n",
       " 694,\n",
       " 90,\n",
       " 1011,\n",
       " 43,\n",
       " 504,\n",
       " 15,\n",
       " 12,\n",
       " 11,\n",
       " 6,\n",
       " 6,\n",
       " 10,\n",
       " 2102,\n",
       " 38,\n",
       " 110,\n",
       " 69,\n",
       " 23,\n",
       " 110,\n",
       " 271,\n",
       " 9,\n",
       " 49,\n",
       " 526,\n",
       " 136,\n",
       " 15,\n",
       " 153,\n",
       " 996,\n",
       " 4,\n",
       " 3,\n",
       " 18,\n",
       " 1,\n",
       " 69,\n",
       " 71,\n",
       " 841,\n",
       " 101,\n",
       " 654,\n",
       " 1252,\n",
       " 8,\n",
       " 549,\n",
       " 6,\n",
       " 9,\n",
       " 65,\n",
       " 93,\n",
       " 9,\n",
       " 325,\n",
       " 149,\n",
       " 164,\n",
       " 341,\n",
       " 148,\n",
       " 24,\n",
       " 74,\n",
       " 154,\n",
       " 73,\n",
       " 19,\n",
       " 32,\n",
       " 11,\n",
       " 17,\n",
       " 144,\n",
       " 81,\n",
       " 38,\n",
       " 992,\n",
       " 79,\n",
       " 174,\n",
       " 262,\n",
       " 4,\n",
       " 1336,\n",
       " 96,\n",
       " 304,\n",
       " 87,\n",
       " 224,\n",
       " 327,\n",
       " 277,\n",
       " 31,\n",
       " 22,\n",
       " 66,\n",
       " 58,\n",
       " 96,\n",
       " 63,\n",
       " 7,\n",
       " 202,\n",
       " 3,\n",
       " 164,\n",
       " 175,\n",
       " 25,\n",
       " 105,\n",
       " 32,\n",
       " 169,\n",
       " 1199,\n",
       " 46,\n",
       " 86,\n",
       " 6,\n",
       " 594,\n",
       " 15,\n",
       " 132,\n",
       " 388,\n",
       " 137,\n",
       " 116,\n",
       " 161,\n",
       " 2,\n",
       " 1,\n",
       " 72,\n",
       " 84,\n",
       " 245,\n",
       " 711,\n",
       " 37,\n",
       " 528,\n",
       " 5,\n",
       " 171,\n",
       " 83,\n",
       " 53,\n",
       " 168,\n",
       " 347,\n",
       " 44,\n",
       " 1070,\n",
       " 104,\n",
       " 80,\n",
       " 12,\n",
       " 309,\n",
       " 50,\n",
       " 2,\n",
       " 109,\n",
       " 763,\n",
       " 60,\n",
       " 616,\n",
       " 218,\n",
       " 87,\n",
       " 36,\n",
       " 20,\n",
       " 299,\n",
       " 160,\n",
       " 190,\n",
       " 284,\n",
       " 20,\n",
       " 722,\n",
       " 237,\n",
       " 489,\n",
       " 254,\n",
       " 37,\n",
       " 143,\n",
       " 36,\n",
       " 448,\n",
       " 15,\n",
       " 811,\n",
       " 200,\n",
       " 332,\n",
       " 364,\n",
       " 8,\n",
       " 1849,\n",
       " 305,\n",
       " 187,\n",
       " 307,\n",
       " 264,\n",
       " 186,\n",
       " 24,\n",
       " 27,\n",
       " 479,\n",
       " 1200,\n",
       " 5,\n",
       " 510,\n",
       " 83,\n",
       " 713,\n",
       " 15,\n",
       " 545,\n",
       " 51,\n",
       " 4,\n",
       " 525,\n",
       " 15,\n",
       " 316,\n",
       " 7,\n",
       " 35,\n",
       " 67,\n",
       " 46,\n",
       " 1,\n",
       " 65,\n",
       " 311,\n",
       " 215,\n",
       " 64,\n",
       " 43,\n",
       " 171,\n",
       " 119,\n",
       " 41,\n",
       " 59,\n",
       " 92,\n",
       " 624,\n",
       " 6,\n",
       " 177,\n",
       " 243,\n",
       " 23,\n",
       " 61,\n",
       " 83,\n",
       " 24,\n",
       " 78,\n",
       " 80,\n",
       " 109,\n",
       " 590,\n",
       " 121,\n",
       " 426,\n",
       " 27,\n",
       " 92,\n",
       " 73,\n",
       " 286,\n",
       " 127,\n",
       " 22,\n",
       " 854,\n",
       " 868,\n",
       " 52,\n",
       " 1128,\n",
       " 186,\n",
       " 53,\n",
       " 2,\n",
       " 381,\n",
       " 68,\n",
       " 50,\n",
       " 97,\n",
       " 65,\n",
       " 155,\n",
       " 272,\n",
       " 506,\n",
       " 177,\n",
       " 58,\n",
       " 2,\n",
       " 58,\n",
       " 1243,\n",
       " 198,\n",
       " 197,\n",
       " 449,\n",
       " 107,\n",
       " 666,\n",
       " 307,\n",
       " 5,\n",
       " 510,\n",
       " 55,\n",
       " 2,\n",
       " 246,\n",
       " 83,\n",
       " 74,\n",
       " 38,\n",
       " 1,\n",
       " 27,\n",
       " 34,\n",
       " 2,\n",
       " 7,\n",
       " 7,\n",
       " 348,\n",
       " 129,\n",
       " 873,\n",
       " 1053,\n",
       " 19,\n",
       " 124,\n",
       " 219,\n",
       " 105,\n",
       " 65,\n",
       " 6,\n",
       " 55,\n",
       " 65,\n",
       " 303,\n",
       " 91,\n",
       " 364,\n",
       " 51,\n",
       " 17,\n",
       " 89,\n",
       " 118,\n",
       " 22,\n",
       " 6,\n",
       " 243,\n",
       " 131,\n",
       " 245,\n",
       " 50,\n",
       " 14,\n",
       " 381,\n",
       " 41,\n",
       " 226,\n",
       " 792,\n",
       " 4,\n",
       " 6,\n",
       " 9,\n",
       " 48,\n",
       " 900,\n",
       " 289,\n",
       " 389,\n",
       " 19,\n",
       " 432,\n",
       " 5,\n",
       " 47,\n",
       " 103,\n",
       " 7,\n",
       " 383,\n",
       " 59,\n",
       " 1067,\n",
       " 18,\n",
       " 47,\n",
       " 6,\n",
       " 19,\n",
       " 1032,\n",
       " 5,\n",
       " 149,\n",
       " 18,\n",
       " 416,\n",
       " 32,\n",
       " 195,\n",
       " 1,\n",
       " 226,\n",
       " 157,\n",
       " 383,\n",
       " 68,\n",
       " 42,\n",
       " 44,\n",
       " 138,\n",
       " 66,\n",
       " 211,\n",
       " 126,\n",
       " 11,\n",
       " 2,\n",
       " 1889,\n",
       " 795,\n",
       " 1345,\n",
       " 98,\n",
       " 180,\n",
       " 53,\n",
       " 1537,\n",
       " 55,\n",
       " 621,\n",
       " 134,\n",
       " 579,\n",
       " 13,\n",
       " 15,\n",
       " 77,\n",
       " 22,\n",
       " 31,\n",
       " 4,\n",
       " 124,\n",
       " 296,\n",
       " 125,\n",
       " 396,\n",
       " 259,\n",
       " 6,\n",
       " 853,\n",
       " 45,\n",
       " 42,\n",
       " 146,\n",
       " 278,\n",
       " 51,\n",
       " 42,\n",
       " 429,\n",
       " 4,\n",
       " 23,\n",
       " 151,\n",
       " 38,\n",
       " 258,\n",
       " 184,\n",
       " 290,\n",
       " 1,\n",
       " 2,\n",
       " 77,\n",
       " 132,\n",
       " 159,\n",
       " 176,\n",
       " 61,\n",
       " 399,\n",
       " 180,\n",
       " 183,\n",
       " 381,\n",
       " 43,\n",
       " 379,\n",
       " 89,\n",
       " 14,\n",
       " 2,\n",
       " 721,\n",
       " 39,\n",
       " 203,\n",
       " 71,\n",
       " 9,\n",
       " 309,\n",
       " 142,\n",
       " 36,\n",
       " 33,\n",
       " 20,\n",
       " 162,\n",
       " 221,\n",
       " 45,\n",
       " 871,\n",
       " 563,\n",
       " 45,\n",
       " 4,\n",
       " 31,\n",
       " 153,\n",
       " 13,\n",
       " 28,\n",
       " 163,\n",
       " 1296,\n",
       " 755,\n",
       " 88,\n",
       " 18,\n",
       " 23,\n",
       " 460,\n",
       " 39,\n",
       " 581,\n",
       " 164,\n",
       " 19,\n",
       " 322,\n",
       " 59,\n",
       " 215,\n",
       " 33,\n",
       " 45,\n",
       " 155,\n",
       " 1,\n",
       " 55,\n",
       " 183,\n",
       " 88,\n",
       " 44,\n",
       " 31,\n",
       " 6,\n",
       " 540,\n",
       " 44,\n",
       " 322,\n",
       " 1,\n",
       " 48,\n",
       " 125,\n",
       " 28,\n",
       " 94,\n",
       " 383,\n",
       " 324,\n",
       " 68,\n",
       " 425,\n",
       " 172,\n",
       " 682,\n",
       " 33,\n",
       " 149,\n",
       " 53,\n",
       " 83,\n",
       " 92,\n",
       " 1365,\n",
       " 17,\n",
       " 383,\n",
       " 133,\n",
       " 178,\n",
       " 220,\n",
       " 30,\n",
       " 92,\n",
       " 4,\n",
       " 1322,\n",
       " 223,\n",
       " 1469,\n",
       " 205,\n",
       " 157,\n",
       " 3,\n",
       " 64,\n",
       " 154,\n",
       " 1276,\n",
       " 12,\n",
       " 21,\n",
       " 1,\n",
       " 28,\n",
       " 26,\n",
       " 349,\n",
       " 33,\n",
       " 3,\n",
       " 92,\n",
       " 93,\n",
       " 29,\n",
       " 23,\n",
       " 371,\n",
       " 35,\n",
       " 186,\n",
       " 18,\n",
       " 71,\n",
       " 189,\n",
       " 8,\n",
       " 136,\n",
       " 9,\n",
       " 2,\n",
       " 87,\n",
       " 29,\n",
       " 85,\n",
       " 8,\n",
       " 124,\n",
       " 34,\n",
       " 14,\n",
       " 217,\n",
       " 33,\n",
       " 51,\n",
       " 31,\n",
       " 210,\n",
       " 195,\n",
       " 10,\n",
       " 158,\n",
       " 32,\n",
       " 17,\n",
       " 9,\n",
       " 41,\n",
       " 110,\n",
       " 1472,\n",
       " 22,\n",
       " 32,\n",
       " 144,\n",
       " 147,\n",
       " 21,\n",
       " 77,\n",
       " 100,\n",
       " 1322,\n",
       " 35,\n",
       " 1279,\n",
       " 12,\n",
       " 84,\n",
       " 48,\n",
       " 144,\n",
       " 103,\n",
       " 498,\n",
       " 45,\n",
       " 413,\n",
       " 27,\n",
       " 7,\n",
       " 66,\n",
       " 1,\n",
       " 88,\n",
       " 1578,\n",
       " 183,\n",
       " 173,\n",
       " 10,\n",
       " 581,\n",
       " 447,\n",
       " 78,\n",
       " 182,\n",
       " 655,\n",
       " 91,\n",
       " 41,\n",
       " 87,\n",
       " 7,\n",
       " 307,\n",
       " 136,\n",
       " 77,\n",
       " 249,\n",
       " 204,\n",
       " 88,\n",
       " 27,\n",
       " 9,\n",
       " 31,\n",
       " 100,\n",
       " 36,\n",
       " 64,\n",
       " 133,\n",
       " 433,\n",
       " 34,\n",
       " 47,\n",
       " 96,\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.eigenvector_centrality()\n",
    "G.pagerank()\n",
    "G.degree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "** Todo: **\n",
    "- Decide which users you will analyze\n",
    "- Compute the centrality for each user\n",
    "- Compute the topic distribution for each user. \n",
    "- Analyze whether there is a relation between the two measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "centrality_values = G.eigenvector_centrality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = [] # centrality\n",
    "b = [] # topic entropy\n",
    "\n",
    "for i, v in enumerate(G.vs):\n",
    "    if v['name'] not in selected_users_sampled: # author_reply_to\n",
    "        continue\n",
    "        \n",
    "    a.append(centrality_values[i])\n",
    "    row = topic_sentiment_user_df.loc[topic_sentiment_user_df['author_from'] ==  v['name']].values.tolist()[0]\n",
    "    topic_distribution =  row[-15:]\n",
    "    \n",
    "    b.append(scipy.stats.entropy(topic_distribution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpearmanrResult(correlation=0.22448061248061255, pvalue=6.8864898434479773e-13)\n"
     ]
    }
   ],
   "source": [
    "print scipy.stats.spearmanr(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Sentiment and centrality \n",
    "\n",
    "In order to become central in the commenter network, sufficient people have to respond to your comment. Enticing others to respond is thus essential. This is more likely when comments are controversial: i.e. many people would disagree with the comment. What is controversial depends on in which environment a statement is made. At any rate, we could expect a controversial statement to be met with criticism. We should then expect that central people are more likely to be criticised, and that they attract relatively many negative comments.\n",
    "\n",
    "Techniques necessary\n",
    "- Sentiment analysis\n",
    "- Centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Sentiment analysis **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from empath import Empath\n",
    "lexicon = Empath()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at post number 340"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What nonsense. The country is called \"The United States of America\". States' rights are an integral part of the US and have been so ever since it's existence.\n",
      "\n",
      "What exactly is supposed bad about states' rights? Are you one of those globalist \"world government\" loonies?\n"
     ]
    }
   ],
   "source": [
    "print df.iloc[[340]]['comment'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the comment using Empath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return list(gensim.utils.simple_preprocess(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'achievement': 0.0,\n",
       " 'affection': 0.0,\n",
       " 'aggression': 0.0,\n",
       " 'air_travel': 0.0,\n",
       " 'alcohol': 0.0,\n",
       " 'ancient': 0.0,\n",
       " 'anger': 0.0,\n",
       " 'animal': 0.0,\n",
       " 'anonymity': 0.0,\n",
       " 'anticipation': 0.0,\n",
       " 'appearance': 0.0,\n",
       " 'art': 0.0,\n",
       " 'attractive': 0.0,\n",
       " 'banking': 0.0,\n",
       " 'beach': 0.0,\n",
       " 'beauty': 0.0,\n",
       " 'blue_collar_job': 0.0,\n",
       " 'body': 0.0,\n",
       " 'breaking': 0.0,\n",
       " 'business': 0.0,\n",
       " 'car': 0.0,\n",
       " 'celebration': 0.0,\n",
       " 'cheerfulness': 0.0,\n",
       " 'childish': 0.0,\n",
       " 'children': 0.0,\n",
       " 'cleaning': 0.0,\n",
       " 'clothing': 0.0,\n",
       " 'cold': 0.0,\n",
       " 'college': 0.0,\n",
       " 'communication': 0.0,\n",
       " 'competing': 0.0,\n",
       " 'computer': 0.0,\n",
       " 'confusion': 0.0,\n",
       " 'contentment': 0.0,\n",
       " 'cooking': 0.0,\n",
       " 'crime': 0.0,\n",
       " 'dance': 0.0,\n",
       " 'death': 0.0,\n",
       " 'deception': 0.0,\n",
       " 'disappointment': 0.0,\n",
       " 'disgust': 0.0,\n",
       " 'dispute': 0.0,\n",
       " 'divine': 0.0,\n",
       " 'domestic_work': 0.0,\n",
       " 'dominant_heirarchical': 0.022222222222222223,\n",
       " 'dominant_personality': 0.0,\n",
       " 'driving': 0.0,\n",
       " 'eating': 0.0,\n",
       " 'economics': 0.0,\n",
       " 'emotional': 0.022222222222222223,\n",
       " 'envy': 0.0,\n",
       " 'exasperation': 0.0,\n",
       " 'exercise': 0.0,\n",
       " 'exotic': 0.0,\n",
       " 'fabric': 0.0,\n",
       " 'family': 0.0,\n",
       " 'farming': 0.0,\n",
       " 'fashion': 0.0,\n",
       " 'fear': 0.0,\n",
       " 'feminine': 0.0,\n",
       " 'fight': 0.0,\n",
       " 'fire': 0.0,\n",
       " 'friends': 0.0,\n",
       " 'fun': 0.0,\n",
       " 'furniture': 0.0,\n",
       " 'gain': 0.0,\n",
       " 'giving': 0.0,\n",
       " 'government': 0.044444444444444446,\n",
       " 'hate': 0.022222222222222223,\n",
       " 'healing': 0.0,\n",
       " 'health': 0.0,\n",
       " 'hearing': 0.0,\n",
       " 'help': 0.0,\n",
       " 'heroic': 0.0,\n",
       " 'hiking': 0.0,\n",
       " 'hipster': 0.0,\n",
       " 'home': 0.0,\n",
       " 'horror': 0.0,\n",
       " 'hygiene': 0.0,\n",
       " 'independence': 0.0,\n",
       " 'injury': 0.0,\n",
       " 'internet': 0.0,\n",
       " 'irritability': 0.0,\n",
       " 'journalism': 0.0,\n",
       " 'joy': 0.0,\n",
       " 'kill': 0.0,\n",
       " 'law': 0.022222222222222223,\n",
       " 'leader': 0.022222222222222223,\n",
       " 'legend': 0.0,\n",
       " 'leisure': 0.0,\n",
       " 'liquid': 0.0,\n",
       " 'listen': 0.0,\n",
       " 'love': 0.0,\n",
       " 'lust': 0.0,\n",
       " 'magic': 0.0,\n",
       " 'masculine': 0.0,\n",
       " 'medical_emergency': 0.0,\n",
       " 'medieval': 0.0,\n",
       " 'meeting': 0.0,\n",
       " 'messaging': 0.0,\n",
       " 'military': 0.022222222222222223,\n",
       " 'money': 0.0,\n",
       " 'monster': 0.0,\n",
       " 'morning': 0.0,\n",
       " 'movement': 0.0,\n",
       " 'music': 0.0,\n",
       " 'musical': 0.0,\n",
       " 'negative_emotion': 0.022222222222222223,\n",
       " 'neglect': 0.0,\n",
       " 'negotiate': 0.0,\n",
       " 'nervousness': 0.0,\n",
       " 'night': 0.0,\n",
       " 'noise': 0.0,\n",
       " 'occupation': 0.0,\n",
       " 'ocean': 0.0,\n",
       " 'office': 0.0,\n",
       " 'optimism': 0.0,\n",
       " 'order': 0.0,\n",
       " 'pain': 0.022222222222222223,\n",
       " 'party': 0.0,\n",
       " 'payment': 0.0,\n",
       " 'pet': 0.0,\n",
       " 'philosophy': 0.0,\n",
       " 'phone': 0.0,\n",
       " 'plant': 0.0,\n",
       " 'play': 0.0,\n",
       " 'politeness': 0.0,\n",
       " 'politics': 0.0,\n",
       " 'poor': 0.0,\n",
       " 'positive_emotion': 0.0,\n",
       " 'power': 0.0,\n",
       " 'pride': 0.0,\n",
       " 'prison': 0.0,\n",
       " 'programming': 0.0,\n",
       " 'rage': 0.0,\n",
       " 'reading': 0.0,\n",
       " 'real_estate': 0.0,\n",
       " 'religion': 0.0,\n",
       " 'restaurant': 0.0,\n",
       " 'ridicule': 0.0,\n",
       " 'royalty': 0.0,\n",
       " 'rural': 0.0,\n",
       " 'sadness': 0.0,\n",
       " 'sailing': 0.0,\n",
       " 'school': 0.0,\n",
       " 'science': 0.0,\n",
       " 'sexual': 0.0,\n",
       " 'shame': 0.022222222222222223,\n",
       " 'shape_and_size': 0.0,\n",
       " 'ship': 0.0,\n",
       " 'shopping': 0.0,\n",
       " 'sleep': 0.0,\n",
       " 'smell': 0.0,\n",
       " 'social_media': 0.0,\n",
       " 'sound': 0.0,\n",
       " 'speaking': 0.0,\n",
       " 'sports': 0.0,\n",
       " 'stealing': 0.0,\n",
       " 'strength': 0.0,\n",
       " 'suffering': 0.022222222222222223,\n",
       " 'superhero': 0.0,\n",
       " 'surprise': 0.0,\n",
       " 'swearing_terms': 0.022222222222222223,\n",
       " 'swimming': 0.0,\n",
       " 'sympathy': 0.0,\n",
       " 'technology': 0.0,\n",
       " 'terrorism': 0.0,\n",
       " 'timidity': 0.0,\n",
       " 'tool': 0.0,\n",
       " 'torment': 0.0,\n",
       " 'tourism': 0.0,\n",
       " 'toy': 0.0,\n",
       " 'traveling': 0.0,\n",
       " 'trust': 0.0,\n",
       " 'ugliness': 0.0,\n",
       " 'urban': 0.0,\n",
       " 'vacation': 0.0,\n",
       " 'valuable': 0.0,\n",
       " 'vehicle': 0.0,\n",
       " 'violence': 0.022222222222222223,\n",
       " 'war': 0.0,\n",
       " 'warmth': 0.0,\n",
       " 'water': 0.0,\n",
       " 'weakness': 0.0,\n",
       " 'wealthy': 0.0,\n",
       " 'weapon': 0.0,\n",
       " 'weather': 0.0,\n",
       " 'wedding': 0.0,\n",
       " 'white_collar_job': 0.0,\n",
       " 'work': 0.0,\n",
       " 'worship': 0.0,\n",
       " 'writing': 0.0,\n",
       " 'youth': 0.0,\n",
       " 'zest': 0.0}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon.analyze(tokenize(df.iloc[[340]]['comment'].values[0]), normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we have precomputed the sentiment values (but if you have time: extend it and consider other features as well,\n",
    "                                                like emotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are going to look at responses, we first join the dataset to have access to the author_reply_to field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_combined = df.join(topic_sentiment_df, lsuffix='_orig', rsuffix='_nlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             comment  a_score  a_created_utc  \\\n",
      "0  You think these women were sitting out there m...        0     1476397749   \n",
      "1                               Oh wow, so profound!        0     1476298220   \n",
      "2                   Hillary will be your president.         0     1477809663   \n",
      "3  Notice that after this one, Pence stated and I...        0     1476072979   \n",
      "4  People are already required to file their tax ...        0     1475538428   \n",
      "\n",
      "   a_retrieved_on comment_id_orig comment_reply_to_id author_from_orig  \\\n",
      "0      1478583791         d8qwv4k             d8qwg9k   Schmingleberry   \n",
      "1      1478553301         d8p5x7o             d8p5ch9     socoamaretto   \n",
      "2      1478983377         d9dpj9r             d9dpf06     allisslothed   \n",
      "3      1478488885         d8lh0lw             d8lfk96     shaking_head   \n",
      "4      1478338625         d8cu7q1             d8ctw14         InFearn0   \n",
      "\n",
      "        author_reply_to comment_id_nlp author_from_nlp    ...          t_5  \\\n",
      "0  pm_me_your_cuck_pics        d8qwv4k  Schmingleberry    ...     0.001667   \n",
      "1            Rustyastro        d8p5x7o    socoamaretto    ...     0.016667   \n",
      "2         PineappleBoss        d9dpj9r    allisslothed    ...     0.033333   \n",
      "3     TheDudeNeverBowls        d8lh0lw    shaking_head    ...     0.002899   \n",
      "4             Muppetude        d8cu7q1        InFearn0    ...     0.038145   \n",
      "\n",
      "        t_6       t_7       t_8       t_9      t_10      t_11      t_12  \\\n",
      "0  0.058648  0.001667  0.001667  0.146286  0.164797  0.001667  0.109014   \n",
      "1  0.016667  0.766667  0.016667  0.016667  0.016667  0.016667  0.016667   \n",
      "2  0.033333  0.033333  0.033333  0.033333  0.033333  0.033333  0.533333   \n",
      "3  0.002899  0.156079  0.002899  0.072449  0.002899  0.002899  0.118771   \n",
      "4  0.001515  0.001515  0.032549  0.001515  0.112769  0.152212  0.060566   \n",
      "\n",
      "       t_13      t_14  \n",
      "0  0.061628  0.126965  \n",
      "1  0.016667  0.016667  \n",
      "2  0.033333  0.033334  \n",
      "3  0.002899  0.002899  \n",
      "4  0.194684  0.001515  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "print df_combined.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very similar to what we did before. Compute the mean for each author (but now we are looking at responses, so we look at 'author_reply_to')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_combined_author_reply_to = df_combined.groupby(['author_reply_to'], as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_reply_to</th>\n",
       "      <th>a_score</th>\n",
       "      <th>a_created_utc</th>\n",
       "      <th>a_retrieved_on</th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "      <th>t_0</th>\n",
       "      <th>t_1</th>\n",
       "      <th>t_2</th>\n",
       "      <th>t_3</th>\n",
       "      <th>...</th>\n",
       "      <th>t_5</th>\n",
       "      <th>t_6</th>\n",
       "      <th>t_7</th>\n",
       "      <th>t_8</th>\n",
       "      <th>t_9</th>\n",
       "      <th>t_10</th>\n",
       "      <th>t_11</th>\n",
       "      <th>t_12</th>\n",
       "      <th>t_13</th>\n",
       "      <th>t_14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81933</th>\n",
       "      <td>sprcow</td>\n",
       "      <td>6.511111</td>\n",
       "      <td>1.477001e+09</td>\n",
       "      <td>1.478755e+09</td>\n",
       "      <td>0.015939</td>\n",
       "      <td>0.009377</td>\n",
       "      <td>0.041849</td>\n",
       "      <td>0.044994</td>\n",
       "      <td>0.04556</td>\n",
       "      <td>0.16002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020855</td>\n",
       "      <td>0.062705</td>\n",
       "      <td>0.100086</td>\n",
       "      <td>0.082979</td>\n",
       "      <td>0.022196</td>\n",
       "      <td>0.034937</td>\n",
       "      <td>0.09735</td>\n",
       "      <td>0.099477</td>\n",
       "      <td>0.048544</td>\n",
       "      <td>0.045009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      author_reply_to   a_score  a_created_utc  a_retrieved_on       pos  \\\n",
       "81933          sprcow  6.511111   1.477001e+09    1.478755e+09  0.015939   \n",
       "\n",
       "            neg       t_0       t_1      t_2      t_3    ...          t_5  \\\n",
       "81933  0.009377  0.041849  0.044994  0.04556  0.16002    ...     0.020855   \n",
       "\n",
       "            t_6       t_7       t_8       t_9      t_10     t_11      t_12  \\\n",
       "81933  0.062705  0.100086  0.082979  0.022196  0.034937  0.09735  0.099477   \n",
       "\n",
       "           t_13      t_14  \n",
       "81933  0.048544  0.045009  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined_author_reply_to[df_combined_author_reply_to['author_reply_to'] == 'sprcow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = [] # centrality\n",
    "b = [] # sentiment - pos\n",
    "c = [] # sentiment - neg\n",
    "d = []\n",
    "for i, v in enumerate(G.vs):\n",
    "    if v['name'] not in selected_users_sampled: # author_reply_to\n",
    "        continue\n",
    "        \n",
    "    a.append(centrality_values[i])\n",
    "    row = df_combined_author_reply_to.loc[df_combined_author_reply_to['author_reply_to'] ==  v['name']]\n",
    "    pos = row['pos'].values[0]\n",
    "    neg = row['neg'].values[0]\n",
    "    \n",
    "    b.append(pos)\n",
    "    c.append(neg)\n",
    "    d.append(pos+neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpearmanrResult(correlation=0.11766470166470168, pvalue=0.0001920947192013784)\n",
      "SpearmanrResult(correlation=0.072682116682116688, pvalue=0.021529011579097989)\n",
      "SpearmanrResult(correlation=0.091321303321303324, pvalue=0.0038492035148635631)\n"
     ]
    }
   ],
   "source": [
    "print scipy.stats.spearmanr(a,b)\n",
    "print scipy.stats.spearmanr(a,c)\n",
    "print scipy.stats.spearmanr(a,d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Communities of interest\n",
    "\n",
    "Earlier today you learned that interaction is often homophilous: people with the same interest are more likely to be connected to each other. We will look into this question here on the basis of topics. Two question are central in this assignment: (1) are users that share topics more likely to be connected; and (2) does this create communities of interest.\n",
    "\n",
    "Techniques necessary\n",
    "- Topic modelling\n",
    "- Assortativity\n",
    "- Community detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The most difficult part of community detection is deciding what method is appropriate and sometimes what resolution is appropriate. First we need to import the library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import louvain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modularity is the most often used and you can get is using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Attribute does not exist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-659e0507a2f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpartition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlouvain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_partition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Modularity'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weight'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/dongnguyen/Programs/anaconda/envs/4tuseminar_python2/lib/python2.7/site-packages/louvain/functions.pyc\u001b[0m in \u001b[0;36mfind_partition\u001b[0;34m(graph, method, initial_membership, weight, resolution_parameter, consider_comms)\u001b[0m\n\u001b[1;32m    187\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m       \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m       \u001b[0;31m# Make sure it is a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Attribute does not exist'"
     ]
    }
   ],
   "source": [
    "partition = louvain.find_partition(G, 'Modularity', weight='weight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can try out CPM, using various resolution values. Good resolution values are usually quite small, but this may depend on the weight. Try to search for a resolution parameter that gives you a solution in the range of 5-50 communities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Attribute does not exist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-884d47df720d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpartition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlouvain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_partition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CPM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weight'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresolution_parameter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.08\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/dongnguyen/Programs/anaconda/envs/4tuseminar_python2/lib/python2.7/site-packages/louvain/functions.pyc\u001b[0m in \u001b[0;36mfind_partition\u001b[0;34m(graph, method, initial_membership, weight, resolution_parameter, consider_comms)\u001b[0m\n\u001b[1;32m    187\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m       \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m       \u001b[0;31m# Make sure it is a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Attribute does not exist'"
     ]
    }
   ],
   "source": [
    "partition = louvain.find_partition(G, 'CPM', weight='weight', resolution_parameter=0.08)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment and language across communities\n",
    "\n",
    "Following social balance theory, it is possible that the commenter network is highly polarized (not implausible given the divisive US politics). Simply looking at communication while disregarding the valence of the link (i.e. whether it was negative or positive) may distort our view of the integration of the network. We will use sentiment analysis of the comments to determine whether the links are in fact negative or positive. In this assignment two question are central: (1) does the valence of links change the community structure; and (2) is sentiment different within sentiment different from language between groups?\n",
    "TODO: Social balance\n",
    "\n",
    "Techniques necessary\n",
    "- Sentiment analysis\n",
    "- Community detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Bonus assignment: social influence\n",
    "\n",
    "Earlier you studied homophily: are users that share interest more likely to connect? In this assignment you will study whether social influence takes place. The goal is to study whether new words are more likely to be used by some user, if another user mentioned this word to him earlier. Note that you explicitly need to take into account the time dimension in this assignment. You should calculate two probabilities: (1) the probability to use a new word given it was not observed before versus (2) the probability to use a new word given it was observed before. Given your results, do you think there is social influence, or do you have another explanation?\n",
    "\n",
    "Techniques\n",
    "- Topic detection\n",
    "- Sentiment analysis\n",
    "- Community detection\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
